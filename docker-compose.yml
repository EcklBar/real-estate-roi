x-airflow-common: &airflow-common
  build: ./airflow
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db:5432/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    MINIO_ENDPOINT: minio:9000
    MINIO_ACCESS_KEY: minioadmin
    MINIO_SECRET_KEY: minioadmin123
    WAREHOUSE_HOST: postgres-warehouse
    WAREHOUSE_PORT: 5432
    WAREHOUSE_DB: nadlanist
    WAREHOUSE_USER: nadlanist
    WAREHOUSE_PASSWORD: nadlanist123
  volumes:
    - ./dags:/opt/airflow/dags
    - ./src:/opt/airflow/src
    - ./processing:/opt/airflow/processing
    - ./jars:/opt/airflow/jars
    - airflow-logs:/opt/airflow/logs
  depends_on:
    airflow-db:
      condition: service_healthy
    postgres-warehouse:
      condition: service_healthy
    minio:
      condition: service_healthy

services:

  minio:
    image: minio/minio:latest
    container_name: nadlanist-minio
    ports: 
      - "9000:9000"
      - "9001:9001"
    environment: 
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck: 
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 5
    
  minio-init: 
    image: minio/mc:latest
    container_name: nadlanist-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: > 
      /bin/sh -c "
      mc alias set local http://minio:9000 minioadmin minioadmin123;
      mc mb --ignore-existing local/nadlanist-raw;
      mc mb --ignore-existing local/nadlanist-clean;
      echo 'Buckets created successfully!';
      "

  postgres-warehouse: 
    image: postgis/postgis:16-3.4
    container_name: nadlanist-warehouse
    ports: 
      - "5433:5432"
    environment:
      POSTGRES_DB: nadlanist
      POSTGRES_USER: nadlanist
      POSTGRES_PASSWORD: nadlanist123
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./include/sql/init.sql:/docker-entrypoint-initdb.d/01-init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nadlanist -d nadlanist"]
      interval: 10s
      timeout: 5s
      retries: 5

  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: nadlanist-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-data:/var/lib/zookeeper/data

  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: nadlanist-kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    depends_on:
      - zookeeper
    volumes:
      - kafka-data:/var/lib/kafka/data
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 15s
      timeout: 10s
      retries: 5

  kafdrop:
    image: obsidiandynamics/kafdrop:latest
    container_name: nadlanist-kafdrop
    ports:
      - "9003:9000"
    environment:
      KAFKA_BROKERCONNECT: kafka:29092
    depends_on:
      - kafka

  spark-master:
    image: apache/spark:3.5.4
    container_name: nadlanist-spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./processing:/opt/spark-jobs
      - ./jars:/opt/spark/jars/custom

  spark-worker:
    image: apache/spark:3.5.4
    container_name: nadlanist-spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    environment:
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
    depends_on:
      - spark-master
    volumes:
      - ./processing:/opt/spark-jobs
  
  airflow-db:
    image: postgres:16-alpine
    container_name: nadlanist-airflow-db
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    volumes:
      - airflow-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 10s
      timeout: 5s
      retries: 5
  airflow-init:
    <<: *airflow-common
    container_name: nadlanist-airflow-init
    entrypoint: /bin/bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@nadlanist.local && echo 'Airflow initialized!'"
    restart: "no"

  airflow-webserver:
    <<: *airflow-common
    container_name: nadlanist-airflow-webserver
    command: webserver
    ports:
      - "8080:8080"
    restart: always

  airflow-scheduler:
    <<: *airflow-common
    container_name: nadlanist-airflow-scheduler
    command: scheduler
    restart: always
  
  dashboard:
    build:
      context: ./dashboard
      dockerfile: Dockerfile
    container_name: nadlanist-dashboard
    ports:
      - "8501:8501"
    environment:
      WAREHOUSE_HOST: postgres-warehouse
      WAREHOUSE_PORT: 5432
      WAREHOUSE_DB: nadlanist
      WAREHOUSE_USER: nadlanist
      WAREHOUSE_PASSWORD: nadlanist123
      KAFKA_BOOTSTRAP_SERVERS: kafka:29092
      DIROBOT_BASE_URL: https://api.dirobot.co.il/api/v2
    depends_on:
      postgres-warehouse:
        condition: service_healthy
    volumes:
      - ./dashboard:/app
  
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: nadlanist-elasticsearch
    ports:
      - "9200:9200"
    environment:
      discovery.type: single-node
      xpack.security.enabled: 'false'
      ES_JAVA_OPTS: "-Xms512m -Xmx512m"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    profiles:
      - elk

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: nadlanist-kibana
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_HOSTS: http://elasticsearch:9200
    depends_on:
      - elasticsearch
    profiles:
      - elk

volumes:
  minio-data:
  postgres-data:
  kafka-data:
  zookeeper-data:
  airflow-db-data:
  airflow-logs:
  elasticsearch-data: